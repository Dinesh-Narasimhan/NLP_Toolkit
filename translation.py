# -*- coding: utf-8 -*-
"""translation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MRK5ZuwgjBtbXfgWmzhn5pSH3i_ck9HT
"""

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

MODEL_NAME = "facebook/nllb-200-distilled-600M"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False, cache_dir="models/")
model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME, cache_dir="models/")

LANGUAGES = {
    "english": "eng_Latn",
    "hindi": "hin_Deva",
    "tamil": "tam_Taml",
    "telugu": "tel_Telu",
    "malayalam": "mal_Mlym",
    "kannada": "kan_Knda",
    "japanese": "jpn_Jpan",
    "spanish": "spa_Latn",
    "french": "fra_Latn"
}

def translate_text(text, source_lang, target_lang):
    if source_lang not in LANGUAGES or target_lang not in LANGUAGES:
        return "Unsupported language"

    src_code = LANGUAGES[source_lang]
    tgt_code = LANGUAGES[target_lang]

    tokenizer.src_lang = src_code
    encoded = tokenizer(text, return_tensors="pt")
    target_lang_token_id = tokenizer.convert_tokens_to_ids(tgt_code)
    generated_tokens = model.generate(**encoded,forced_bos_token_id=target_lang_token_id)
    output = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
    return output[0]

